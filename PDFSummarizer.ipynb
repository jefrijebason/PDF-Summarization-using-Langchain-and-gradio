{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNO622FHxa5LclOiKfd56P1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sudarshan-koirala/youtube-stuffs/blob/main/langchain/PDFSummarizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PDF Summarizer with few lines of code using Gradio, OpenAI and LangChain"
      ],
      "metadata": {
        "id": "yTdPFcqYlvTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install necessary packages"
      ],
      "metadata": {
        "id": "6wL3laYImIY-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- https://pypdf.readthedocs.io/en/stable/index.html\n",
        "- https://www.gradio.app/\n",
        "- https://github.com/openai/tiktoken\n",
        "- https://docs.langchain.com/docs/"
      ],
      "metadata": {
        "id": "EHm1oFMc12zd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_EkR21whlkn6"
      },
      "outputs": [],
      "source": [
        "#install necessary packages\n",
        "!pip install -q gradio openai pypdf tiktoken langchain"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#with open('env_vars.json', 'r') as f:\n",
        "#    env_vars = json.load(f)\n",
        "#openai.api_key = env_vars[\"OPENAI_API_KEY\"]"
      ],
      "metadata": {
        "id": "1w19ZNKxr37l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://platform.openai.com/account/api-keys\n",
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"OPENAI_API_KEY\""
      ],
      "metadata": {
        "id": "vY7HQRdMpmAO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\n",
        "import tiktoken\n",
        "\n",
        "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
        "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
        "    encoding = tiktoken.get_encoding(encoding_name)\n",
        "    print(encoding.encode(string))\n",
        "    num_tokens = len(encoding.encode(string))\n",
        "    return num_tokens\n",
        "\n",
        "num_tokens_from_string(\"tiktoken is great!\", \"cl100k_base\")"
      ],
      "metadata": {
        "id": "eVPEZ7bt2vGR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6b83af3-ffe3-4286-ff73-1900317ea5bf"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[83, 1609, 5963, 374, 2294, 0]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "from langchain import OpenAI, PromptTemplate\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "\n",
        "llm = OpenAI(temperature=0)"
      ],
      "metadata": {
        "id": "gMgR1njymD8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#PyPDFLoader??"
      ],
      "metadata": {
        "id": "sHEA4UF_qix4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LangChain part \n",
        "#### Function that takes PDF file as input and returns the summary of that PDF\n",
        "- langchain `PyPDFLoader` helps load the PDF\n",
        "- After that we can split the document in smaller chunks\n",
        "- We then use the `load_summarize_chain` to create a summarization chain\n",
        "- Langchain covers three different chain types: stuff, map_reduce, and refine. We will use `map_reduce` for this example. Refer --> [Langchain summarization](https://python.langchain.com/en/latest/modules/chains/index_examples/summarize.html)"
      ],
      "metadata": {
        "id": "bDE4Dm99qdYk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# just to show you how it works\n",
        "loader = PyPDFLoader('/content/2023_GPT4All_Technical_Report.pdf')\n",
        "doc=loader.load_and_split()\n",
        "print(len(doc))\n",
        "doc[0]"
      ],
      "metadata": {
        "id": "MCO5KwFRxd7Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb342f8d-8ed4-47ac-d1fd-4dd0d17b1cac"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content='GPT4All: Training an Assistant-style Chatbot with Large Scale Data\\nDistillation from GPT-3.5-Turbo\\nYuvanesh Anand\\nyuvanesh@nomic.aiZach Nussbaum\\nzanussbaum@gmail.com\\nBrandon Duderstadt\\nbrandon@nomic.aiBenjamin Schmidt\\nben@nomic.aiAndriy Mulyar\\nandriy@nomic.ai\\nAbstract\\nThis preliminary technical report describes the\\ndevelopment of GPT4All, a chatbot trained\\nover a massive curated corpus of assistant in-\\nteractions including word problems, story de-\\nscriptions, multi-turn dialogue, and code. We\\nopenly release the collected data, data cura-\\ntion procedure, training code, and final model\\nweights to promote open research and repro-\\nducibility. Additionally, we release quantized\\n4-bit versions of the model allowing virtually\\nanyone to run the model on CPU.\\n1 Data Collection and Curation\\nWe collected roughly one million prompt-\\nresponse pairs using the GPT-3.5-Turbo OpenAI\\nAPI between March 20, 2023 and March 26th,\\n2023. To do this, we first gathered a diverse sam-\\nple of questions/prompts by leveraging three pub-\\nlicly available datasets:\\n• The unified chip2 subset of LAION OIG.\\n• Coding questions with a random sub-sample\\nof Stackoverflow Questions\\n• Instruction-tuning with a sub-sample of Big-\\nscience/P3\\nWe chose to dedicate substantial attention to data\\npreparation and curation based on commentary in\\nthe Stanford Alpaca project (Taori et al., 2023).\\nUpon collection of the initial dataset of prompt-\\ngeneration pairs, we loaded data into Atlas for data\\ncuration and cleaning. With Atlas, we removed all\\nexamples where GPT-3.5-Turbo failed to respond\\nto prompts and produced malformed output. This\\nreduced our total number of examples to 806,199\\nhigh-quality prompt-generation pairs. Next, we\\ndecided to remove the entire Bigscience/P3 sub-\\nset from the final training dataset due to its very\\nFigure 1: TSNE visualization of the candidate training\\ndata (Red: Stackoverflow, Orange: chip2, Blue: P3).\\nThe large blue balls (e.g. indicated by the red arrow)\\nare highly homogeneous prompt-response pairs.\\nlow output diversity; P3 contains many homoge-\\nneous prompts which produce short and homoge-\\nneous responses from GPT-3.5-Turbo. This exclu-\\nsion produces a final subset containing 437,605\\nprompt-generation pairs, which is visualized in\\nFigure 2. You can interactively explore the dataset\\nat each stage of cleaning at the following links:\\n• Cleaned with P3\\n• Cleaned without P3 (Final Training Dataset)\\n2 Model Training\\nWe train several models finetuned from an in-\\nstance of LLaMA 7B (Touvron et al., 2023).\\nThe model associated with our initial public re-\\nlease is trained with LoRA (Hu et al., 2021)\\non the 437,605 post-processed examples for four\\nepochs. Detailed model hyper-parameters and\\ntraining code can be found in the associated repos-\\nitory and model training log.', metadata={'source': '/content/2023_GPT4All_Technical_Report.pdf', 'page': 0})"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_pdf(pdf_file_path):\n",
        "    loader = PyPDFLoader(pdf_file_path)\n",
        "    docs = loader.load_and_split()\n",
        "    chain = load_summarize_chain(llm=llm, chain_type=\"map_reduce\")\n",
        "    summary = chain.run(docs)   \n",
        "    return summary"
      ],
      "metadata": {
        "id": "RZX-spcApXXm"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lets grab the document\n",
        "!wget https://s3.amazonaws.com/static.nomic.ai/gpt4all/2023_GPT4All_Technical_Report.pdf"
      ],
      "metadata": {
        "id": "AKKYr2yB4Gor"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summarize = summarize_pdf(\"/content/2023_GPT4All_Technical_Report.pdf\")\n",
        "summarize"
      ],
      "metadata": {
        "id": "4N0oxt_9tGgQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "f43462de-e8d1-4764-e586-d0ea796f84e6"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' This paper presents GPT4All, a chatbot trained on a large curated dataset of assistant interactions. The authors provide all data, training code, and model weights for the community to use and evaluate the model using human evaluation data from the Self-Instruct paper. The paper also references three models (LORA, Stanford Alpaca, and LLAMA) and Self-Instruct, which are all designed to be low-rank, open, and efficient language models. The data and training details are released to accelerate open LLM research.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a simple gradio UI (if you prefer UI)"
      ],
      "metadata": {
        "id": "PXEcdNg5rKDi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_pdf(pdf_file_path):\n",
        "    loader = PyPDFLoader(pdf_file_path)\n",
        "    docs = loader.load_and_split()\n",
        "    chain = load_summarize_chain(llm=llm, chain_type=\"map_reduce\")\n",
        "    summary = chain.run(docs)   \n",
        "    return summary"
      ],
      "metadata": {
        "id": "SdIE8OTj-NuB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_pdf_path = gr.components.Textbox(label=\"Provide the PDF file path\")\n",
        "output_summary = gr.components.Textbox(label=\"Summary\")\n",
        "\n",
        "interface = gr.Interface(\n",
        "    fn=summarize_pdf,\n",
        "    inputs=input_pdf_path,\n",
        "    outputs=output_summary,\n",
        "    title=\"PDF Summarizer\",\n",
        "    description=\"Provide PDF file path to get the summary.\",\n",
        ").launch(share=True)"
      ],
      "metadata": {
        "id": "Oth5CY7BrFH4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        },
        "outputId": "dcf6085c-1c22-4eec-a6ce-79a5867d3e1f"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://0928952a4c9150f59d.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades (NEW!), check out Spaces: https://huggingface.co/spaces\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://0928952a4c9150f59d.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "By4cEj7LJ7_Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}